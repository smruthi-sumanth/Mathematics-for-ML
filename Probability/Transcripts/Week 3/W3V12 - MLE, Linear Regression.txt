Recall that a way that maximum likelihood is used in machine learning
is the following. We have a bunch of data and some possible models that could
have generated that data. For each one of the models, you calculate the
probability that the data appears
based on Model 1, Model 2, and Model 3. The one that gives the highest probability
is the winner, that means you find
the model that most likely produced
in that data. The one with the highest
probability is the winner, that's the one that maximizes
P of data given model. Here's a small example
with linear regression. Let's say that these are your points and you're trying
to fit a line to them, but let's not try to
fit a line to them. Let's try to do it
with probability. The candidates are Model
1, that's this line, Model 2 which is this line, and Model 3 which is this line. First of all, let's think, which one looks
like the best fit? The best fit is going
to be the one that's going to give the
highest probability. The first one generates the
data with some probability, the second one with some
higher probability, and the third one with
some medium probability. The one that wins is this one. However, I haven't yet told you how a line can
generate points. Let me tell you that next. The idea is that these lines generate
points close to the line. Imagine it as a road. If you have that road being the yellow line and you
start building houses, the houses are
likely to be built close to the road
and not far from it. The road is Model 2 then the houses are likely
to be in these points, and If the road is Model
3 then the houses are likely to be around here. But, let me put
some numbers here. Let's say that this is your
model and this is a point x, and the point of
line is over here. You're going to generate a point close to the
line using a Gaussian. We put a Gaussian, and we center it at the point where the line meets
the horizontal line. We're going to sample
out of that Gaussian, so we sample this one over here. That's the same as saying, we're going to
build a house that is likely to be close to the road and it's going to be sampled from that vertical
Gaussian over there, if we have a bunch of
points x_1 up to x_5, then we have five Gaussians all centered at the point
on the blue line. From each one of these, we're going to
sample some point. Those are the five
points that we're sampling based on
this blue line. Now, all we have to do is find the line that best
produce the points, the one that most likely
produce these points. Coincidentally,
that's going to be the same best fit of a line
from linear regression. Why is this the case? Well, let's do some math. Let's say that the line
has equation y = mx+b, and these are the points
of intersection with the line and these are
the generated points. These distances are
going to be called d_1, d_2, d_3, d_4, and d_5. First, let's calculate the likelihood of
generating this point. Since we have a Gaussian, then what really happens is that the Gaussian is centered at the point where x_1
intersects that blue line, and we're generating these
other point over here. If we assume that
it's a Gaussian with standard deviation
one and mean equal 0, then the likelihood of
generating these point is given by the formula of the Gaussian which is 1/square
root of 2Pi(e^-1/2d_1^2). That is the Likelihood of
draining the first point. We're looking at all of them. That's the product of
the five likelihoods. That's what we have to maximize. We have to maximize the likelihood of the line
generating all those points. Since they're independent, then we have to look
at the product. A bunch of these
things are constants. This 1/square root 2Pi. Let's forget about them, and let's maximize
this over here. We can factor this as e^-1/2 times the sum of the
squares of the d heights. Notice that if we
maximize something, that's the same as maximizing the logarithm so we
can get rid of this e, and also to the 1/2 because that would just
be multiplying by 2. But, let's remember that we
have a negative number here. That means we have to
minimize the sum of squares. Minimizing the sum of squares is precisely the least
square error. This is exactly what
linear regression is. In linear regression, you want to find
the blue line that minimizes the sum of squared
distances from the point. This shows that finding the line that most likely
produce a point using maximum likelihood
is exactly the same as minimizing the
least square error using linear regression. Here are three examples of lines and the Gaussians that
degenerate at each point. As you can see, when we calculate
the likelihoods for the actual dataset
of blue points, the winning one is the
one in the middle, which we can see is the best
line fit for the dataset.