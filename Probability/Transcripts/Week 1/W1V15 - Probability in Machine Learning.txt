So you may be wondering why are we
talking so much about probabilities and what do they have to do
with machine learning? Well, it turns out that machine
learning is a lot about probabilities. Many times in machine learning
what you want to do is, you want to calculate a probability of
something given some other factors. So for example, in spam detection,
you try to calculate the probability that an email is spam based on
the words on the email or the recipients or the attachment or
other features of the email. This is a conditional probability because
probability of spam given some features. Another example is sentiment analysis
where you want to determine if a piece of text is happy or sad. In this case, you want to find
the probability that the piece of text is happy given the words it contains. Let's do another example,
image recognition. In here you try to find out if an image
has a particular thing or not. Let's say that you want to recognize
if there's a cat in an image or not. So you calculate the probability
that there's a cat in the image based on
the pixels in the image. So these are all
conditional probabilities. However, pure probabilities also
appear a lot in machine learning. There's another big area of machine
learning called generative machine learning and it's a part of unsupervised
machine learning where you want to maximize probabilities. So for example, image generation, if
you've seen these great images of faces, for example, that are generated by
computers, in here you want to maximize the probability that a bunch
of pixels form a human face. Or in text generation,
you want to maximize the probability that a bunch of words are sensical text
and that it talks about a certain thing. So all of these are examples of machine
learning that use a lot of probability and let me elaborate on them now. So in the previous videos you got
to see Bayes' theorem in action, where first you find the prior
the probability that an email is spam, but just the initial probability, namely dividing the number of spam emails
divided by the total number of emails. Then there was an event, for example, the
email contains the word lottery and then a posterior which refined this probability
by creating a tree of possibilities. This gave us four possibilities that
the email was spam and lottery, that it was spam and no lottery,
that it was ham and lottery, and that it was ham and no lottery. And then, you further calculated
the probability of spam and lottery by forgetting all the emails
that don't contain the word lottery and doing the calculation there. Then the probability of spam given lottery
is equal to the probability of spam and lottery divided by the sum of
the probabilities of spam and lottery and ham and lottery and
that was the posterior. But when you look at this in high level,
what really happened is that you created a machine learning
classifier by something that gives you a calculation of a probability
of a thing given another thing. And that is what machine
learning is in many, many cases. Imagine image recognition. Let's say that an image recognition
classifier tells you if an image contains a cat or not. What this is really doing is telling
you the probability that you have a cat in an image based on some events, and
the events are the pixels in the image. So a classifier will tell you
the probability of cat given pixel one, pixel two, all the way to pixel n. This is an image classifier. Another example would be
in the medical field. Let's say that you have a bunch of
demographics and symptoms of the patient and a lot of metrics, and you want to
know if the patient is healthy or not. So what you do is you calculate the
probability that the patient is healthy based on their symptoms and their history. So you build a model that calculates
this conditional probability. So here's another example,
sentiment analysis. In sentiment analysis, what you do
is you train a model that will tell you if a given sentence is happy or sad. And what you're doing here is
calculating the probability, the conditional probability that
an email is happy based on some events. And the events are the words
in the sentence. So let's consider the image recognition
problem to see how this works. What you do is you train some model,
and this is the model, and the model what it does is it takes
an image, so a bunch of pixels, and tells you the probability that
the image has a cat given the pixels. In this case it would be
0.9 because it's a cat. And if you give us something different,
say a car, then it says, well, the probability
that you have a cat here, given the pixels is
going to be very small. So let's say 0.1. So you determine that this is not a cat. So, as you see, machine learning is all
about finding conditional probabilities. In particular, this is supervised
machine learning because you're answering questions about the data. For example, does the image contain a cat? Is the sentence happy? Is this email spam? Etc. Another very interesting example of
conditional probabilities are generative models, for example,
face generation models. The idea is to train a model that can
generate a group of pixels that result in an image that looks like a human face. This is done by trying to achieve a high
probability of the image being a face given the generated pixels. For example, the image on
the right is not a real person. It was generated by
a model called a StyleGAN. I don't know about you,
but this really fooled me. That face looks very real.