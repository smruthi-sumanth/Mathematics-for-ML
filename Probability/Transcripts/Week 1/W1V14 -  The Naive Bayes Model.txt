So let's go back to the Bayes' theorem
example where we had the prior, the probability that an email is spam,
the event that gives us information about the email, and the posterior, which is a
new, better probability that the email is spam given the event, which is
that it contains the word lottery. Now obviously, we're not going to
build a classifier with only one word. We're going to use it with many,
many words. So let's look at another one. Let's say that we use the word winning and
we calculate the posterior, probability of an email being spam given the fact
that it contains the word winning. Now how would we combine these two? How would we make a stronger classifier
with the words lottery and winning? The idea would be to
find the posterior here. So we can do it in a simple way by simply
calculating the probability that an email is spam given the fact that it
contains lottery and winning, by dividing the number of spam
emails that contain lottery and winning by the number of total emails
that contain lottery and winning. In other words, if this is Bayes' theorem, we'd be calculating this number as the
number of spam emails that contain the two words divided by the total
number of spam emails. But can you see a problem there? Well, imagine if I don't want to do
this with 2 words but with 100 words, w1 up to w100. So here's Bayes' theorem. And if I want to calculate this,
then I need to divide the number of emails that contain all 100 words divided
by the number of spam emails. However, asking for an email to
contain 100 words is very hard. Maybe there's no emails like that in
our database, and so we get 0 over 0. And that's not a good calculation. So what can we do? Well, let's go back to
the problem with two words. And the idea is that we really
want to calculate these. And if we can't calculate them,
then we can at least estimate them. And we're going to estimate
them using a naïve assumption. The naïve assumption is
the key to naïve Bayes, and that's why it's called naïve Bayes. And the assumption says that
the appearances of the words lottery and winning are independent from each other. Now this is obviously not true. Words are not independent from each other,
words depend on each other. If I have the word good appearing, well,
the word morning is more likely to appear because good morning
tend to appear together. So words are dependent. But if we assume that they're independent,
then the math works out much, much better. And we actually can get some pretty good
results by making this naïve assumption. So that's why we build
a naïve Bayes classifier. So when you look at probability of
lottery and winning given spam, that's the probability of
an intersection of two things. And if they happen to be independent, then their probability is
the product of the two things. So all of a sudden we have probability
of lottery and winning given spam. We have probability of lottery giving spam
times probability of winning given spam. And the same thing for
the denominator for spam and for ham. And the same thing happens with n words. N can be huge, but the probability of
an email containing all these words given that is spam can be estimated as
the product of all these probabilities of the email containing each one
of the words given that it's spam. And that is the naïve Bayes algorithm. That is the calculation for naïve Bayes. Let's look at an example. Let's go back to the previous example
where we had that there's 20 spam emails out of 100, and therefore the prior is
that the probability of spam is 20%. The probability of ham, which is not spam,
is 80%, because it's 100 minus 20%. And because among the 20 spam emails
14 of them contain the word lottery, then the probability of lottery given
spam is 14 over 20, which is 0.7. And because there were 10 ham emails
containing the word lottery out of 80 total ham emails, then the probability of
lottery given ham is 10 over 80 or 0.125. When we do the same thing
with the word winning, we see that there are 15 spam emails
that contain the word winning and only 8 ham emails that
contain the word winning. So p of winning given spam
is 15 over 20 or 0.75, and p of winning given ham is 8 over 80,
which is 0.1. So with these numbers,
we are going to do the calculation. So here is what we had for naïve Bayes. And when we get the calculation with these
numbers, we get that the probability of an email being spam given that
it contains the words lottery and winning is 0.2 times 0.7 times 0.75,
divided by 0.2 times 0.7 times 0.75, + 0.8 times 0.125 times 0.1,
which is 0.913. And as you can imagine, if an email
contains the words lottery and win, it's much, much more likely to be spam. So this 91.3% is much higher. So that is the naïve Bayes algorithm is
very, very useful and very powerful. And in a lab,
you're going to be able to play with it.