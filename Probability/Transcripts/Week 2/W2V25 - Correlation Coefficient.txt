Now, consider these two data sets shown
here that you've already seen before with their variances and their covariances. We know that the covariance tells us
how the variations of the two variables are related. When this is a positive value it
tells us that both variables tend to increase together. And when it's negative it tells
us that when one increases, the other one decreases. But notice that one is a bigger
number than the other one. Forget about the signs,
one is 17 and the other one is 7.45. Does that mean that the correlation
is stronger for the one that is 17? Well, we cannot really tell because the
covariance can assume any value with no limited range. So consider these two data sets shown
here that you've already seen before, age versus naps and age versus height. Age versus naps has
a negative covariance and age versus height has
a positive covariance. And the reason is because the more
you age, the less naps you take, and in here the more you age,
the more you grow. However, because of 17 is bigger than
7.45, forgetting about the sign, does that mean that
the correlation here is stronger? And we can't really tell that because
it could be that the numbers in the right are just much bigger numbers. So the question is how could we tell
the actual correlation between the two? So what I'm going to show you is something
called a correlation coefficient. The correlation coefficient is
a number that lies between -1 and 1. -1 is when you have two variables that
are completely negatively correlated. Positive one is when you have two
variables that are completely positively correlated and
0 is when they're completely independent. And it's always a number between 1 and -1. So these two numbers don't really fit in
here, but after standardizing they will. So let me show you the formula for
the correlation coefficient. And the correlation coefficient
is simply the covariance except divided by sigma x sigma y, so
it's a standardized covariance. We can also write this as covariance of
X,Y divided by square root of variance of X, square root of variance of Y. Now, let's calculate it for
this data set over here. When we take the covariance and
divide it by the square root of the two variances we get negative 0.894,
which is close to -1, which means that these are pretty close to
being completely negatively correlated. And for age versus height then we
have that the correlation is 17 divided by square root of 9.17,
square root of 39.56 and that's 0.893 which means they're
pretty closely correlated. And so we have correlations
coefficients of -0.894 for the one on the left and
0.893 for the one in the right. Notice that despite the big difference
in magnitude of the covariances of these data sets, the magnitude of the
correlation coefficient is always small, it's always between -1 and 1, and
we can see from the graph that it's pretty close to a straight line
from the data points. So the only difference here is
the direction of the diagonal, which shows the sign of
the correlation coefficient. When we calculate the correlation
coefficient for the other example, the one of age and grades,
we actually get a very small number. We get 0.1 divided by the two variances,
which is 0.01, showing that these two
are definitely uncorrelated. Finally, let's calculate the correlation
coefficient for the data and waiting times and customer rating, that is covariance divided by the product
of the square roots of variances, which is -0.845, which shows that it
is indeed pretty negatively correlated. To summarize, the correlation coefficient
is the ratio between the covariance of the two variables and
the product of the standard deviations, that is the product of the square
roots of the variances. It's always between -1 and 1, it's -1 when
we pretty much have the diagonal y = -x. It's 0 when we have something
completely uncorrelated, and it's 1 where we pretty much
have the diagonal y = x. Correlation coefficient is very, very useful in statistics when
you're comparing pairs of variables.