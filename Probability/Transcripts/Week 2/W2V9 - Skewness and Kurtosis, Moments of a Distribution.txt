As you can see, the expected
value and the variance or standard deviation paint a really good picture
of the distribution. However, there are a lot of subtleties that are
not captured by them. Some of them are called
skewness and kurtosis, and let me show
you what they are. Before we get into other ways
to measure distribution, let me tell you
about the moments. It's something
you've already seen, but we can formalize it here. Let's say you have a random
variable that can take the value -2 with
probability 1/3, the value 0 with
probability 1/6, and the value 1 with
probability 1/2. What is the expectation? It's 1/3*-2+1/6*0+1/2*1,
whatever that is. What's the expectation
of the variable squared? Well, it's the same
thing except you take the value squared
times the probability. Now, those are called the
first and the second moment. E[X] is the first moment, the expectation and E[X^2]
is the second moment, which is related to
the variance not exactly the variance
because you have to center, but it's related
to the variance. What do you think goes next? Well, we can continue
with the third moment, which is E[X^3], the expectation of
variable cubed, and so on, and the kth moment is the expectation of the
variable to the k, and those are going to be
pretty useful in a bit. In general, if the
random variable can take the values x_1 up to x_n with probabilities
p_1 up to p_n, then the first moment is sum of p_i x_i the expected value. The second moment is
sum of p_i x_i squared. Third moment is sum
of p_i x_i cubed. The fourth moment is sum of p_i, x_i to the forth and so
on until the kth moment, which is sum of
p_i x_i to the k, and as I said, we're going
to be using those in a bit.