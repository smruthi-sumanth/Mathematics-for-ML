Welcome to week three. In this week, you're going to
learn what a neural network is and how to train them using gradient descent. You're also going to learn a alternate
method to gradiend descent called Newton's method,
which is also very fast and very useful. But let's start with the first video. In this video, you're going to learn the fundamental unit
of neural networks called a perceptron. That may sound like a foreign term to you,
but you've actually seen one all ready, because linear regression can
be expressed as a perceptron. So let's start with the motivation
behind a regression problem. We'll be using one of the most classical
example used in linear regression, predicting the price of a house
given the size of the house. So let's say you have
data on three houses. The first one has 1000 square feet and
it costs $20,000, the second one, 2000 square feet and
a cost of $30,000 and the third house has a size of 3000
square feet and costs $50,000. And let's say that you have more data
points which you plot on this graph. So these are other houses on the market
with their sizes and their prices. And the goal is to take this data of
inputs, which is the size of the house and find in this case a line because they
look like they form some sort of a line. So we're going to find a line used
to predict the price of the house. And the goal is to find
the best possible line. Now in this case we only have one input
type which is a feature that we're going to use to predict the prices. What if we throw in a second feature? So let's throw in another feature
let's say the number of rooms. Now the houses have 2,
4 and 7 rooms respectively. With this added feature,
the problem is not so simple anymore and we need to consider how both
features affect the output. Then how do we build and train a model that is able to make
predictions of the house prices and more. What if we had more features? What it had like 10 features or
100 features? Well here's where the perception comes in. So this regression problem can be
modeled with a single perception. The perception is going to take some
inputs into some unknown outputs that we want to predict. So first we're going to start by
exploring the perception mathematically. So let's start with the inputs. The inputs here are going to be x1 and x2
corresponding to the size of a house and a number of rooms. But I want you to imagine this,
if there were 100 inputs, it would simply be 100 notes
x1 all the way to x100. So we start with the inputs and
then they plug into a summation function. I'll explain more about
this in a little bit. Out of the summation function
comes the output y hat and that's going to be what we predict
to be the the price of the house. So a good model will come out
with pretty good predictions. And the idea is to build
the best possible model. So what is happening in
this summation step? Let me do it more in detail. Each of these features, x1 and
x2 is multiplied by a corresponding weight to determine how
important it is for the output. So for example, if the size of a house
is way more important to predict the price of a house
than the number of rooms, then this will have a higher
weight than the number of rooms. So let's call the corresponding
weights w1 and w2. And to combine these weights and
the input, we simply add them. So we multiply w1 the weight times x1 the feature and we add w2 times x2. And we're almost there. We're not quite done
because there's also a bias term that gets added to
the summation function. And now we add this and
that is the output. In this model there's
no activation function. I will tell you later what
an activation function is. But here the output is simply y hat,
that's w1x1 + w2x2 + b as the predicted
price of the house. And the goal is to find the best w1,
w2 and b the weights and
bias that will optimize predictions. So the ones that will,
when we plug in x1 and x2 give us the closest to the actual
price for the entire data set. Now, how do we do this? How do we find this perfect weights? The idea is that we want to
minimize some kind of error. The error is basically how far
are you from the price of the house? And so if you minimize that error,
then that's all we need. So for that we introduced something
called the loss function. The loss function is a little
function that's going to tell us, how far are we from
predicting the prices well. So that's on the next video.